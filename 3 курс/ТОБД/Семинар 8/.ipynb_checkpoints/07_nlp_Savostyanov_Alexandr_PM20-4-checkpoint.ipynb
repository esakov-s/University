{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы:\n",
    "* Макрушин С.В. Лекция \"Введение в обработку текста на естественном языке\"\n",
    "* https://www.nltk.org/api/nltk.metrics.distance.html\n",
    "* https://pymorphy2.readthedocs.io/en/stable/user/guide.html\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\python\\envs\\python_new\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\python\\envs\\python_new\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\python\\envs\\python_new\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\python\\envs\\python_new\\lib\\site-packages (from pymorphy2) (2.4.417127.4579844)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Вячеслав\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. При помощи расстояния Левенштейна иправьте опечатку в слове \"велечайшим\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import edit_distance"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe8 in position 8: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# считаем файл со словами\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# в начале файла идут пробелы, потом какие-то числа (нам они не нужны), потом сами слова\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitw-win.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# делим строки по пробелам и берем последний элемент\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     words \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fp]\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# считаем файл со словами\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# в начале файла идут пробелы, потом какие-то числа (нам они не нужны), потом сами слова\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitw-win.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# делим строки по пробелам и берем последний элемент\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     words \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fp]\n",
      "File \u001b[1;32mC:\\Python\\envs\\Python_new\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe8 in position 8: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# считаем файл со словами\n",
    "# в начале файла идут пробелы, потом какие-то числа (нам они не нужны), потом сами слова\n",
    "\n",
    "with open(\"litw-win.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    # делим строки по пробелам и берем последний элемент\n",
    "    words = [line.split()[-1] for line in fp]\n",
    "    \n",
    "# это как раз тот случай, когда файл не в utf-8, поэтому ничего не работает\n",
    "# попробуем другую кодировку - windows-1251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"litw-win.txt\", \"r\", encoding=\"windows-1251\") as fp:\n",
    "    # делим строки по пробелам и берем последний элемент\n",
    "    words = [line.split()[-1] for line in fp]\n",
    "# теперь сработало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[10000:10005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для вычисления расстояния Левенштейна есть много разных функций\n",
    "# мы можем воспользоваться nltk, с которым знакомились в прошлый раз\n",
    "\n",
    "# расстояние Левенштейна измеряет близость между строками; чем меньше полученное значение,\n",
    "# тем больше похожи две строки друг на друга\n",
    "# \"похожесть\" измеряется в количестве операций вставки, удаления и замены символов, которые\n",
    "# надо сделать, чтобы одну строку свести к другой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distance(\"Вы справитесь с домашкой\", \"Вы не справитесь с домашкой\")\n",
    "# вернулось число 3, потому что надо после слова \"Вы\" добавить 3 символа: \n",
    "# пробел, \"н\" и \"е\", и тогда две строки станут одинаковыми"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# один из вариантов использования расстояния - исправление ошибок\n",
    "# берем слово с ошибкой\n",
    "word = \"велечайшим\"\n",
    "# у нас есть список words, в котором, мы точно знаем, содержатся слова без ошибок\n",
    "# в списке слов words ищем слово, которое как можно ближе к word\n",
    "\n",
    "# можно воспользоваться встроенным min\n",
    "min(\n",
    "    words,  # ищи минимум по строкам из words\n",
    "    key=lambda w: edit_distance(\n",
    "        word, w\n",
    "    ),  # минимум в смысле расстояния от конкретного слова w до слова word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисление расстояние Левенштейна - в общем случае не дешевая операция\n",
    "# поэтому предыдущая ячейка может занять какое-то время\n",
    "# на практике лучше добавить каких-нибудь эвристик, чтобы сократить объем вычислений\n",
    "# если это возможно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки второго задания на слова. Проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для наглядности чуть поменяю формулировку задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Разбейте текст из формулировки второго задания на слова. Проведите стемминг и лемматизацию слов в тексте.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиваем текст на слова при помощи уже известной нам word_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# при обработке текстов на многих языках возникает одна и та же проблема:\n",
    "# если ничего не предпринять, то различные формы слов будут рассматриваться в нашем\n",
    "# коде, как совершенно разные слова. Например, в words сейчас есть 2 слова: \"текст\" и \"тексте\"\n",
    "# машине неоткуда узнать, что это одно и то же слово, но в разных падежах\n",
    "# если ничего с этим не сделать, то часто можно столкнуться с тем, что:\n",
    "# а) алгоритм будет работать хуже (любой, в т.ч. модель ML)\n",
    "# б) алгоритм будет работать медленнее (т.к. форм слова в русском может быть очень много, размер вашего\n",
    "# словаря слов может быть просто гигантским)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы это победить, есть как минимум 2 варианта: стемминг и лемматизация\n",
    "\n",
    "# начинаем со стемминга. Стемминг - это процедура \"отбрасывания\" последних нескольких символов\n",
    "# слова, которые, по идее, и отвечают на форму. Проблема в том, что заранее вы не знаете,\n",
    "# сколько символов отбросить\n",
    "# русскоязычный стеммер есть в том же nltk\n",
    "\n",
    "# создаем специальный объект-стеммер и указываем,\n",
    "# что будем работать с русским языком (это важно, т.к. другие языки надо обрабатывать по-другому)\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "# для каждого слова из words вызываем метод stem у стеммера\n",
    "{w: stemmer.stem(w) for w in words}\n",
    "\n",
    "# из плюсов - работает достаточно быстро и позволяет решить нашу проблему\n",
    "# из минусов - то, что получилось - это уже не слова русского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лемматизация - это процедура приведения слова к начальной форме. что такое \"начальная форма\" -\n",
    "# зависит от части речи слова (для прилагательных, например, ед.ч именительный пад. мужской род)\n",
    "# в nltk есть инструменты для лемматизации англоязычных слов\n",
    "# а для русского языка мы попользуем пакет pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем специальный объект MorphAnalyzer\n",
    "# важно: его надо создать один раз, и использовать ниже по коду\n",
    "# не создавайте его в циклах и т.д. - это сильно замедлит работу\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем первое слово и посмотрим на его примере, как работать с этой штукой\n",
    "w = words[0]\n",
    "# передаем слово в morph.parse и смотрим на результат\n",
    "res = morph.parse(w)\n",
    "print(res)\n",
    "# результат - это список объектов Parse. Каждый объект - это специальная структура, которая\n",
    "# содержит информацию о том, что это за часть речи, какая у него (слова) нормальная форма\n",
    "# и некоторую другую информацию\n",
    "# почему список? потому что часто pymorphy не может однозначно определить, что это за часть речи\n",
    "# поэтому предлагает нам выбор; этот список упорядочен по убыванию \"правдоподобности\" результата\n",
    "# т.е. часто имеет смысл смотреть на 0й элемент\n",
    "opt1 = res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у объектов Parse есть поле normalized, которое вернем нам тоже объект Parse,\n",
    "# но для нормализованной формы слова\n",
    "print(opt1.normalized)\n",
    "\n",
    "# после этого нам останется только вытащить саму строку из объекта\n",
    "# для этого обратимся к полю word\n",
    "print(opt1.normalized.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь все вместе\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "{w: morph.parse(w)[0].normalized.word for w in words}\n",
    "# опять же, решили нужную нам задачу, но теперь результат - это настоящие слова, что может быть хорошо\n",
    "# для каких-то задач. из минусов - это работает медленнее стемминга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 2 в векторы при помощи `CountVectorizer`. Выведите на экран словарь обученного токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Разбейте текст из формулировки второго задания на слова. Проведите стемминг и лемматизацию слов в тексте.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вы знаете, что модели ML любят числа, не любят буквы\n",
    "# поэтому почти всегда первый наш шаг - преобразовать строку в числовой вектор\n",
    "# в этом месте появляется куча разнообразных методов, решающих эту задачу\n",
    "# здесь мы рассмотрим, наверное, самый простой способ решения этой задачи\n",
    "# на практике так уже не делают, т.к. есть способы кодирования, которые дают более высокие результаты\n",
    "# но это требует уметь работать с нейронками, поэтому, надеюсь, вам это расскажут на курсе по ML\n",
    "\n",
    "# мы вместо сложных нейронок сейчас сделаем следующее: возьмем текст, распилим его на слова, запомним, какие слова в нем есть\n",
    "# каждое слово занумеруем числами от 0 и создадим вектор для каждого предложения по следующему правилу:\n",
    "# сколько раз в предложении встретилось слово с номером i, такое число будет стоять в i-й координате вектора\n",
    "# для данного предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно для этих целей воспользоваться готовым решением из sklearn\n",
    "# считаю, что с sklearn вы уже знакомы из курса ML\n",
    "\n",
    "# нам нужно самим разбить текст на предложения\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "# потом обучаем объект-векторизатор (в задании ошибка: не токенизатор, а векторизатор)\n",
    "# в этом месте происходит разбиение на слова, их нумерация и т.д.\n",
    "cv = CountVectorizer().fit(sents)\n",
    "# и потом преобразуем строки в векторы при помощи метода transform обученного векторизатора\n",
    "# transform вернет sparse массив, сделаем его \"обычным\", вызвав .toarray()\n",
    "sents_cv = cv.transform(sents).toarray()\n",
    "sents_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь как понять, что это за единички и нолики?\n",
    "# посмотрим на поле vocabulary_ у векторизатора\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# видим, что слову \"разбейте\" векторизатор присвоил номер 6 (как и почему он это сделал, сейчас не важно)\n",
    "# смотрим на 6 столбец полученных векторов\n",
    "sents_cv[:, 6]\n",
    "# видим, что в 0 строке стоит 1 (это значит, слово \"разбейте\" встретилось в соответствующем тексте 1 раз)\n",
    "#  а в 1 строке стоит 0 (этого слова там не было)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из плюсов: простота реализации, скорость\n",
    "# из минусов: если слов в словаре много, то векторы становятся очень большие и разреженные \n",
    "# и модели начинают плохо работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обратите внимание, что есть слово \"текст\" с номером 10, а есть слово \"тексте\" с номером 11\n",
    "# получается, что у нас появится две координаты в векторе, хотя по факту слово-то одно\n",
    "# это проблема. ее можно решить при помощи стемминга или лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в домашке вместо такого алгоритма вы будете использовать TF-IDF; \n",
    "# с точки зрения кода это требует от вас взять другой класс из sklearn; \n",
    "# на лекции вам подробно расскажут, чем эти подходы различаются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iteration_utilities\n",
      "  Downloading iteration_utilities-0.11.0-cp38-cp38-win_amd64.whl (89 kB)\n",
      "     ---------------------------------------- 89.1/89.1 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: iteration_utilities\n",
      "Successfully installed iteration_utilities-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install iteration_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import edit_distance\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "from iteration_utilities import flatten\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "pymorphy2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Вячеслав\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Загрузите данные из файла `ru_recipes_sample.csv` в виде `pd.DataFrame` `recipes` Используя регулярные выражения, удалите из описаний (столбец `description`) все символы, кроме русских букв, цифр и пробелов. Приведите все слова в описании к нижнему регистру. Сохраните полученный результат в столбец `description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/164365/</td>\n",
       "      <td>Густой молочно-клубничный коктейль</td>\n",
       "      <td>{'Молоко': '250 мл', 'Клубника': '200 г', 'Сах...</td>\n",
       "      <td>Этот коктейль готовлю из замороженной клубники...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/1306/</td>\n",
       "      <td>Рулетики</td>\n",
       "      <td>{'Сыр твердый': None, 'Чеснок': None, 'Яйцо ку...</td>\n",
       "      <td>Быстро и вкусно.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/10625/</td>\n",
       "      <td>Салат \"Баклажанчик\"</td>\n",
       "      <td>{'Баклажан': '3 шт', 'Лук репчатый': '2 шт', '...</td>\n",
       "      <td>Сытный, овощной салатик, пальчики оближете.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/167337/</td>\n",
       "      <td>Куриные котлеты с картофельным пюре в духовке</td>\n",
       "      <td>{'Фарш куриный': '800 г', 'Пюре картофельное':...</td>\n",
       "      <td>Картофельное пюре и куриные котлеты - вкусная ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/91919/</td>\n",
       "      <td>Рецепт вишневой наливки</td>\n",
       "      <td>{'Вишня': '1 кг', 'Водка': '1 л', 'Сахар': '30...</td>\n",
       "      <td>Вишневая наливка имеет яркий вишневый вкус, ко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/54574/</td>\n",
       "      <td>Мшош</td>\n",
       "      <td>{'Чечевица': '1 стак.', 'Лук репчатый': '2 шт'...</td>\n",
       "      <td>Для тех, кто любит чечевицу... Вам сюда! Очень...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/113494/</td>\n",
       "      <td>Мясные треугольники с баклажаном</td>\n",
       "      <td>{'Фарш мясной': '400 г', 'Баклажан': '1 шт', '...</td>\n",
       "      <td>Баклажановые фантазии продолжаются! Предлагаю ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/83228/</td>\n",
       "      <td>\"Болоньез\" по-новому</td>\n",
       "      <td>{'Фарш мясной': '400 г', 'Томаты в собственном...</td>\n",
       "      <td>Мое любимое блюдо лазанья. Но кушать только фа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/172238/</td>\n",
       "      <td>Варенье из одуванчиков с апельсинами</td>\n",
       "      <td>{'Цветки': '400 г', 'Сахар': '1300 г', 'Апельс...</td>\n",
       "      <td>Прошлым летом варила варенье из одуванчиков по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>https://www.povarenok.ru/recipes/show/52794/</td>\n",
       "      <td>Три корочки хлеба под соусом болоньез для Пино...</td>\n",
       "      <td>{'Хлеб': '3 шт', 'Фарш мясной': '200-250 г', '...</td>\n",
       "      <td>- И три корочки хлеба! - сделал заказ Буратино...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url  \\\n",
       "0     https://www.povarenok.ru/recipes/show/164365/   \n",
       "1       https://www.povarenok.ru/recipes/show/1306/   \n",
       "2      https://www.povarenok.ru/recipes/show/10625/   \n",
       "3     https://www.povarenok.ru/recipes/show/167337/   \n",
       "4      https://www.povarenok.ru/recipes/show/91919/   \n",
       "...                                             ...   \n",
       "3462   https://www.povarenok.ru/recipes/show/54574/   \n",
       "3463  https://www.povarenok.ru/recipes/show/113494/   \n",
       "3464   https://www.povarenok.ru/recipes/show/83228/   \n",
       "3465  https://www.povarenok.ru/recipes/show/172238/   \n",
       "3466   https://www.povarenok.ru/recipes/show/52794/   \n",
       "\n",
       "                                                   name  \\\n",
       "0                    Густой молочно-клубничный коктейль   \n",
       "1                                              Рулетики   \n",
       "2                                   Салат \"Баклажанчик\"   \n",
       "3         Куриные котлеты с картофельным пюре в духовке   \n",
       "4                               Рецепт вишневой наливки   \n",
       "...                                                 ...   \n",
       "3462                                               Мшош   \n",
       "3463                   Мясные треугольники с баклажаном   \n",
       "3464                               \"Болоньез\" по-новому   \n",
       "3465               Варенье из одуванчиков с апельсинами   \n",
       "3466  Три корочки хлеба под соусом болоньез для Пино...   \n",
       "\n",
       "                                            ingredients  \\\n",
       "0     {'Молоко': '250 мл', 'Клубника': '200 г', 'Сах...   \n",
       "1     {'Сыр твердый': None, 'Чеснок': None, 'Яйцо ку...   \n",
       "2     {'Баклажан': '3 шт', 'Лук репчатый': '2 шт', '...   \n",
       "3     {'Фарш куриный': '800 г', 'Пюре картофельное':...   \n",
       "4     {'Вишня': '1 кг', 'Водка': '1 л', 'Сахар': '30...   \n",
       "...                                                 ...   \n",
       "3462  {'Чечевица': '1 стак.', 'Лук репчатый': '2 шт'...   \n",
       "3463  {'Фарш мясной': '400 г', 'Баклажан': '1 шт', '...   \n",
       "3464  {'Фарш мясной': '400 г', 'Томаты в собственном...   \n",
       "3465  {'Цветки': '400 г', 'Сахар': '1300 г', 'Апельс...   \n",
       "3466  {'Хлеб': '3 шт', 'Фарш мясной': '200-250 г', '...   \n",
       "\n",
       "                                            description  \n",
       "0     Этот коктейль готовлю из замороженной клубники...  \n",
       "1                                      Быстро и вкусно.  \n",
       "2           Сытный, овощной салатик, пальчики оближете.  \n",
       "3     Картофельное пюре и куриные котлеты - вкусная ...  \n",
       "4     Вишневая наливка имеет яркий вишневый вкус, ко...  \n",
       "...                                                 ...  \n",
       "3462  Для тех, кто любит чечевицу... Вам сюда! Очень...  \n",
       "3463  Баклажановые фантазии продолжаются! Предлагаю ...  \n",
       "3464  Мое любимое блюдо лазанья. Но кушать только фа...  \n",
       "3465  Прошлым летом варила варенье из одуванчиков по...  \n",
       "3466  - И три корочки хлеба! - сделал заказ Буратино...  \n",
       "\n",
       "[3467 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "recipes = pd.read_csv('ru_recipes_sample.csv', sep=',')\n",
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chk_desc = list(recipes[[\"description\"]].values)\n",
    "# print(*chk_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patt = re.compile(r\"[^0-9а-яА-ЯёЁ ]+\")\n",
    "recipes[\"description\"] = recipes[\"description\"].str.replace(patt, \"\").str.lower()\n",
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chk_desc = list(recipes[[\"description\"]].values)\n",
    "# print(*chk_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patt = re.compile(r\"[^0-9а-яА-ЯёЁ ]+\")\n",
    "# res_chk = recipes[\"description\"].str.findall(patt)\n",
    "# print(*list(res_chk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние Левенштейна. Выведите на экран результат в следующем виде:\n",
    "\n",
    "```\n",
    "d(word1, word2) = x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_words = recipes[\"description\"].apply(lambda x: word_tokenize(x))\n",
    "sep_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(flatten(sep_words)))\n",
    "print(len(unique_words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_unique_words = np.random.choice(words, 10, False).reshape(5, 2)\n",
    "random_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(map(lambda x: f\"d({x[0]}, {x[1]}) = {edit_distance(x[0], x[1])}\", random_unique_words))\n",
    "print(*res, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Напишите функцию, которая принимает на вход 2 текстовые строки `s1` и `s2` и при помощи расстояния Левенштейна определяет, является ли строка `s2` плагиатом `s1`. Функция должна реализовывать следующую логику: для каждого слова `w1` из `s1` проверяет, есть в `s2` хотя бы одно слово `w2`, такое, что расстояние Левенштейна между `w1` и `w2` меньше 2, и считает количество таких слов в `s1` $P$. \n",
    "\n",
    "$$ P = \\#\\{w_1 \\in s_1\\ | \\exists w_2 \\in s_2 : d(w_1, w_2) < tol\\}$$\n",
    "\n",
    "$$ L = max(|s1|, |s2|) $$\n",
    "\n",
    "Здесь $|\\cdot|$ - количество слов в строке, $\\#A$ - число элементов в множестве $A$, $w \\in s$ означает, что слово $w$ содержится в тексте $s$.\n",
    "\n",
    "Если отношение $P / L$ больше 0.8, то функция должна вернуть True; иначе False.\n",
    "\n",
    "Продемонстрируйте работу вашей функции на примере описаний двух рецептов с ID 135488 и 851934 (ID рецепта - это число, стоящее в конце url рецепта). Выведите на экран описания этих рецептов и результат работы функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plagiarism(s1: str, s2: str) -> bool:\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "    lst_words_s1 = tokenizer.tokenize(s1)\n",
    "    lst_words_s2 = tokenizer.tokenize(s2)\n",
    "    p = 0\n",
    "    for w in range(len(lst_words_s1)):\n",
    "        word_to_compare = lst_words_s1[w]\n",
    "        for w_in_s2 in lst_words_s2:\n",
    "            if edit_distance(word_to_compare, w_in_s2) < 2:\n",
    "                p += 1\n",
    "                break\n",
    "                \n",
    "    l = max(len(lst_words_s1), len(lst_words_s2))\n",
    "\n",
    "    return True if p/l > 0.8 else False\n",
    "            \n",
    "ind1 = 135488\n",
    "ind2 = 851934\n",
    "url = recipes[\"url\"].apply(lambda x: x.split('/')[-2])\n",
    "ind_to_choose = list(flatten([url[url == str(ind1)].index.tolist(), \n",
    "                              url[url == str(ind2)].index.tolist()]))\n",
    "    \n",
    "is_plagiarism(recipes.at[ind_to_choose[0], 'description'], \n",
    "              recipes.at[ind_to_choose[1], 'description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. На основе набора слов из задания 2 создайте `pd.DataFrame` со столбцами `word`, `stemmed_word` и `normalized_word`. В столбец `stemmed_word` поместите версию слова после проведения процедуры стемминга; в столбец `normalized_word` поместите версию слова после проведения процедуры лемматизации. Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга можно воспользоваться `SnowballStemmer` из `nltk`, для лемматизации слов - пакетом `pymorphy2`. Сравните результаты стемминга и лемматизации. Поясните на примере одной из строк получившегося фрейма (в виде текстового комментария), в чем разница между двумя этими подходами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "d = {'word': words, 'stemmed_word': [stemmer.stem(w) for w in words], \n",
    "     'normalized_word': [morph.parse(w)[0].normalized.word for w in words]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Добавьте в таблицу `recipes` столбец `description_no_stopwords`, в котором содержится текст описания рецепта после удаления из него стоп-слов. Посчитайте и выведите на экран долю стоп-слов среди общего количества слов. Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "recipes[\"description_no_stopwords\"] = recipes[\"description\"].apply(lambda x: \" \".join([w for w in word_tokenize(x) if w not in russian_stopwords]))\n",
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(flatten(sep_words))\n",
    "fdist_sw_before_del = FreqDist(all_words)\n",
    "most_comm_before_del = fdist_sw_before_del.most_common(10)\n",
    "print(\"Топ-10 самых часто употребляемых слов до удаления стоп слов: \", *most_comm_before_del, sep=\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "words_after_del = recipes[\"description_no_stopwords\"].apply(lambda x: word_tokenize(x))\n",
    "words_after_del = list(flatten(words_after_del))\n",
    "fdist_sw_after_del = FreqDist(words_after_del)\n",
    "most_comm_after_del = fdist_sw_after_del.most_common(10)\n",
    "print(\"\\n\", \"Топ-10 самых часто употребляемых слов после удаления стоп слов: \", *most_comm_after_del, sep=\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "stop_words = [s_w for s_w in all_words if s_w in russian_stopwords]\n",
    "diff = round(len(stop_words)/len(all_words), 3)\n",
    "print(\"\\n\", \"Доля стоп-слов среди общего количества слов :\", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Выберите случайным образом 5 рецептов из набора данных, в названии которых есть слово \"оладьи\" (без учета регистра). Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`. На основе полученных векторов создайте `pd.DataFrame`, в котором названия колонок соответствуют словам из словаря объекта-векторизатора. \n",
    "\n",
    "Примечание: обратите внимание на порядок слов при создании колонок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patt1 = re.compile(r\"оладьи\", re.I)\n",
    "# res1 = recipes[\"name\"].str.findall(patt1)\n",
    "# res1 = res1[res1.apply(len) > 0]\n",
    "# print(len(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=\"english\")\n",
    "# vectorizer.fit(data[\"preprocessed_descriptions\"])\n",
    "\n",
    "# def vectorizer_processing(x):\n",
    "#     sents = [x[\"preprocessed_descriptions\"]]\n",
    "#     vector = vectorizer.transform(sents)\n",
    "#     return vector.toarray()\n",
    "\n",
    "# data['TfidfVectorizer'] = data.apply(lambda x: vectorizer_processing(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_with_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_with_desc = list(flatten(recipes[recipes[\"name\"].str.contains(r\"\\оладьи\\b\", case = False)].sample(5)[[\"description\"]].values))\n",
    "# lst_with_desc\n",
    "\n",
    "cv = CountVectorizer().fit(sents)\n",
    "sents_cv = cv.transform(sents).toarray()\n",
    "sents_cv\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Вычислите близость между каждой парой рецептов, выбранных в задании 6, используя косинусное расстояние (можно воспользоваться функциями из любого пакета: `scipy`, `scikit-learn` или реализовать функцию самому). Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов.\n",
    "\n",
    "Примечание: обратите внимание, что $d_{cosine}(x, x) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напишите функцию, которая принимает на вход `pd.DataFrame`, полученный в задании 7, и возвращает в виде кортежа названия двух различных рецептов, которые являются наиболее похожими. Прокомментируйте результат (в виде текстового комментария). Для объяснения результата сравните слова в описаниях двух этих отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(sim_df: pd.DataFrame) -> tuple:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
